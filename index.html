<!DOCTYPE html>
<html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project
  | CS, Georgia Tech | Fall 2018: CS 4476</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>

<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name -->
<h1>Lane Detection</h1>
<span style="font-size: 20px; line-height: 1.5em;"><strong>Soham Gadgil, Ashwin Natarajan, Shaurye Aggarwal, William Xia, Abhishek Tumuluru, Mohit Chauhan</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2018 CS 4476 Computer Vision: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<!-- Goal -->
<h3>Abstract</h3>
Lane detection is an increasingly relevant topic given the advent of driver-assist technologies, self-driving vehicles, and automation. Implementing a Computer Vision approach in openCV to detect lanes and overlay them is one of the main parts of our project. In addition to pure Computer vision methods, a machine learning method combined with some computer vision is a potential way to solve this problem. We compare the different approaches and point out conditions wherein different approaches perform better.
<br><br>
To compare the two approaches, we use a similar accuracy formula across the approaches to compare the quality of our predicted output to the ground truth that is given along with the dataset. In comparing the two approaches over a variety of condition (curved roads, dark/night pictures, crowded roads, and optimal conditions) we found that the classical computer vision approach yielded consistently better results across all conditions. The time taken by each method on test cases is similar but the computer vision is slightly faster. Hence overall, the classical computer vision model performs better than a machine learning approach in lane detection for the selected dataset.
<br><br>
<!-- figure -->
<!-- <h3>Teaser figure</h3>
A figure that conveys the main idea behind the project or the main application being addressed.
<br><br> -->
<!-- Main Illustrative Figure -->
<div style="text-align: center;">
<img style="height: 250px;" alt="" src="abstract_image.png">
</div>

<br><br>
<!-- Introduction -->
<h3>Introduction</h3>
The goal of our team’s project is divided into three sections. The first is to build a computer vision system that can detect which lane on the road a car is driving on. Secondly, we aim to detect the degree by which the road is turning in the event that there is curvature on the road. The third part of our project is the comparison of the computational performance and outputs of a Machine Learning model versus our classic Computer Vision approach. CNNs, Deep learning, and K-means are potential machine learning models that can be used for this purpose. The input to the classical system will be a short video of a car driving along a road that has clear lane markings on it. The output will be a video of the car driving along the road along with an overlaid image that shows which lane the car is on. For example, if the car is in the middle lane in the video, then the output video would show the car moving with the middle lane having a different color than the other lanes. The output video will also contain a varying numeric value at the top of the screen that indicates how many degrees the road is curving by. This could be a number between 0 and 360 degrees or a number representing the radius of curvature, depending on what gives more information about how much to turn the steering wheel. These two pieces of information (current lane and degree of turning) are the first steps in building an steering system for an autonomous car since the car will be able to steer such that it stays in its current lane, while also adjusting to the curvature of the road. Like the Computer Vision approach, the ML model will take in a video of a car driving along a road and it will return a similar video with highlighted lane markings overlaid on top of the roads in the frames.
<br><br>
<!-- Approach -->
<h3>Approach</h3>
To achieve lane detection our team focuses on two approaches: classical and machine learning methodologies.  Part of our objective is to compare the performance of the two paradigms, namely the speed, accuracy, and resource of the algorithms.  To achieve these comparisons, we measure the performance on a chosen dataset and use both types of approaches to build a better understanding of how quickly each algorithm can accomplish the task.  Speed is measured by how fast each algorithm completes the lane detection, accuracy is measured by how close the algorithms are to ground truth, and the resource consumption is measured by the amount of memory and code length used.
<br><br>

<h4>Classical Approach</h4>
Matching a line to a potential lane involves multiple steps: perspective transform, pixel counting, and spline regression.  We operated with the assumption that the road would be in a certain area in the middle of the image, and we considered this assumption valid since a dashcam will usually be stationarily mounted on the car.  A perspective transform can be applied on this section to emphasize the lines and create a bird’s eye view of the road. To achieve this, we use the getPerspectiveTransform method in openCV which requires a list of the source coordinate points and the corresponding coordinate points in the bird’s eye view. We obtain these correspondences by choosing an area where we expect lanes to be, and map the points to a square.  Next, we threshold the grayscale version of our image so that only very white pixels will be detected; this thresholding is meant to filter out non-lane artifacts, since road lanes are usually painted white.  After the thresholding, we search for pixels in two rectangular areas that represent our guess for where the lane would be located.  As pixels are detected in the rectangles, the rectangles are iteratively adjusted, so that the midpoint of the rectangle is at the location of highest pixel concentration.  Once the boxes reach the top of the image, a path is formed that highlights the path of highest detected pixel density.  The spline is matched according to this path, and is subsequently drawn on the image.  The warped image is then unwarped back to the original perspective, so that the predicted line is overlain on the original image.
<br><br>
The radius of curvature is a measure of how much the road curves.  Since our approach uses a polynomial regression method, it is possible to calculate the radius of curvature once the polynomial is obtained.  We assumed the radius of curvature would be equivalent for both lanes since, usually, driving lanes are parallel to each other, thus we are only outputting one radius of curvature to describe the predicted lane.  The equation for the radius of curvature is as follows:
<div style="text-align: center;">
<img style="height: 100px;" alt="" src="curv_formula.png">
</div>
<br>
Where x1 and x0 are the first and zeroth degree coefficients respectively for the regressed polynomial.
<br><br>

<h4>Machine Learning Approach</h4>

For the machine learning implementation, we chose LaneNet, a deep, branched, multi-task network for lane detection and H-Net as described in [1]. This system claims to be robust to road scene variations unlike many of the classical lane detection techniques. Compared to other machine learning techniques this network doesn’t require predefined and fixed number of lanes in order to detect them and is further robust to lane changes.
<br><br>
The network (and the problem) in this technique is branched into two: <strong> segmentation branch </strong> and <strong> lane embedding branch </strong>. The <strong> segmentation branch </strong> is tasked with deciding whether a pixel belongs to a lane or not and it outputs a binary segmentation map. This part of the network is trained using the standard cross-entropy loss function. Using the output of the segmentation branch, the <strong> lane embedding branch </strong> then disentangles the lane pixels. It outputs an N-channel embedding per pixel where N is the number of embeddings by using the clustering loss function exploited for distance metric learning as proposed in [2]. This branch produces each embedding in such a way such that embeddings of the pixels that are in the same lane are as close as possible and the embeddings of the pixels belonging to different lanes as far as possible. Due to this fact, the pixels of the same lanes are then clustered using an iterative process that ends when all pixels have been assigned to a lane.
<br><br>
Given an input image, the networks outputs a map where each lane pixel is labeled with a lane id. To actually fit curves to the lane, instead of using the standard “bird-eye” transformation the system employs H-Net which outputs the optimal transformation H given an input image. This makes the curve-fitting robust to ground-plane changes. The transformation H is then used to transform each lane pixel and fit a 3rd order polynomial. Each lane’s polynomial is then projected back into the image space.

<br><br>
<!-- Results -->
<h3>Experiments and results</h3>
<h4> Experiments </h4>
Since the primary focus of the project is to find the optimal lane detection algorithm and comparing the two different approaches used, most of the experiments involved running the algorithms on test datasets and analyzing and comparing the results for areas of improvement. We looked into test video frames for 4 types of conditions: <br>
<ol>
  <li> <strong> Optimal Lanes </strong>-  Good lighting conditions, well-marked lanes which remain relatively straight for a long time (such as a highway), no interruptions between lanes (such as a pedestrian crossing). Such videos are expected to result in good accuracy in detecting lanes.
  <li> <strong> Crowded Roads </strong>- Many cars on the road resulting in possible lane interruptions
  <li> <strong> Curved Lanes </strong>- Lanes curving on a highway, or lanes which curve at a turnaround
  <li> <strong> Dark Conditions </strong>- Driving at night resulting in poor lighting conditions and reflections from the oncoming cars with the headlights on
</ol>
<br>
This project compared two approaches of detecting lanes. The first one is the classical approach involving image manipulation and feature detection. The classical approach did not need a dataset per se, since there is no “learning” involved. The second one was using Machine Learning models to train using labelled data and to detect lanes in unseen data. Collecting our own data was a challenge so we relied on existing datasets to provide us with our training data. We used the CULane dataset [3], which is a large scale dataset geared toward research on lane detection. It includes more that 55 hours of video with the extraction of 133235 frames. The dataset is manually annotated for each frame and the test set is divided into normal and challenging categories. We further divided that dataset into subsets of the above mentioned categories. Each subset had the following number of frames:
<br><br>
<div style="text-align: center;">
<img style="height: 150px;" alt="" src="dataset_split.png">
</div>
<br><br>
In the classical approach, we had to make some decisions regarding the hyperparameter values. The parameters were set as follows:
<ol>
  <li> <strong> Matrices for perspective transform: </strong> The destination matrix contains the points corresponding to the points in the source matrix. The matrix values are shown below. Each row in the source matrix maps to the same row in the destination matrix. <br>
    <div style="text-align: center;">
    <img style="height: 150px;" alt="" src="source_matrix.png">
    <img style="height: 150px;" alt="" src="dest_matrix.png">
    </div>
    <br>
  </li>
  <li>
    <strong>Threshold for Binarization: </strong> The first step in the classical approach was converting the grayscale image obtained to a binary image by thresholding it. The threshold was applied to the histogram equalized equivalent of the grayscale image. Since each type of lane condition was different because of the environment and lane structure, the threshold value became an important hyperparameter. We tested each type of lane condition with three different thresholds values and compared the F-Scores for each value to determine the threshold value that worked best for each condition. The threshold values tested for each conditions were as follows:
    <br><br>
    <div style="text-align: center;">
    <img style="height: 150px;" alt="" src="optimal_thresh.png">
    </div>
  </li>
</ol>
<br><br>
In the machine learning approach there are not many parameters that need tweaking. We obtained the implementation for the LaneNet model on <a href = https://github.com/MaybeShewill-CV/lanenet-lane-detection> Github </a> . We further used their pre-trained model that was trained on the TuSimple Data Set. Next we used a subset of CULane Dataset to get preliminary results and accuracy.
<br><br>
To evaluate our results in terms of accuracy, we chose F-score. To evaluate our results in terms of accuracy, we chose F-score. F-score is the harmonic mean of the precision and recall. Our F-score was calculated as such:
<br><br>
<div style="text-align: center;">
<img style="height: 75px;" alt="" src="fscore.png">
</div>
Where
<div style="text-align: center;">
<img style="height: 150px;" alt="" src="precision.png">
</div>
<br>
The range of F-score is [0, 1], where an F-score of 1 implies perfect classification, and an f-score of 0 implies no correct classifications.
<br><br>
We chose to use f-score because our problem focused on binary classification of each pixel as a lane or not a lane. Furthermore, f-score takes into account both precision and recall, as opposed to some other metrics as sum of squared errors.
<br><br>
To account for scaling issues and errors in possible minor errors in polynomial curve fitting, we allowed a margin of error of 30 pixels from the ground truth.
The different conditions of the images contribute to the variance in F-scores.  Curved lanes are images where the lanes are significantly curved compared to the other datasets, crowded roads consist of images that are cluttered with other objects such as cars, dark conditions are when the images are taken in low-light, and optimal lanes are the best, clearest images of lanes we could find in the dataset.
<br><br>
<h4> Results </h4>
The following graph shows the F-Score variation with respect to the threshold for the classical approach for each condition:
<div style="text-align: center;">
<img style="height: 600px;" alt="" src="chart_1.png">
</div>
For the Curved Lanes graph, the thresholds tested do not affect the F-score significantly.  The Crowded Roads graph shows significant change to the F-score as the threshold is varied.  For the Dark Conditions graph, the thresholds tested do not vary the F-score significantly.  The Optimal Lanes graph indicates that for these sets of test images the F-score increased as the threshold increased.
<br><br>
The thresholds determine what grayscale value could be considered a lane.  With optimal and curved lane graphs, the images were clear and the lanes could be easily seen, thus as long as the threshold value was not significantly lower than white, the F-score would remain mostly the same since the lanes would be easily detected.  For dark conditions, the lane line is hard to discern from the surrounding image as well as glare from lamps and oncoming cars whilst driving in the dark.  Thus the threshold does not have much of an affect on the image since detecting the lanes in grayscale is difficult due to the conditions.  For crowded roads, there are more than just the lane lines in the image, so thresholding is much more important for this set of data.  When the road is crowded with other cars, the algorithm may mistake a part of a car for a lane, but alternatively if the thresholding is too high the lane may not be detected properly, and this sensitivity causes the large fluctuation in F-score we see in the graph.
<br><br>
The following graph shows the F-Scores of the two approaches for each lane condition:
<div style="text-align: center;">
<img style="height: 400px;" alt="" src="chart_2.png">
</div>
To compare the F-scores across the Machine Learning Model and Classical Approach, we averaged out the F-score obtained across the thresholds for the classical approach. For both the approaches, the variation over the different conditions was as expected. The F-Score was higher for the curved and the optimal lanes since there was not much noise due to traffic as in the case of crowded roads and the environment was was not as bad as in the case of the dark conditions. Also, the dark conditions performed the worst with the lowest F-Score as expected since these conditions have a lot of noise coming from glares
The ML approach generally had worse performance than the classical approach.  This result could be attributed to the binary lane segmentation branch of the model(1) ; since the binary lane segmentation depends on how easily the lane is differentiated from the surrounding environment, a cluttered lane image or a road with bad lighting will cause the algorithm to have difficulty discerning the lane from the background.  Thus not only will the detection be negatively affected by bad data, but the training of the algorithm will be similarly impacted.
<br><br>
The speed for both approaches are given in the table below (in seconds):
<br><br>
<div style="text-align: center;">
<img style="height: 150px;" alt="" src="speed_table.png">
</div>

<!-- Results -->
<h3>Qualitative results</h3>
<h4>Classical Approach Results </h4>
<h5> Crowded Roads</h5>
<div style="text-align: center;">
<img style="height: 500px;" alt="" src="crowded_classical.png">
</div>
<br>
For crowded roads, as seen in the results section, the optimal threshold is 200, as the F score indicates. Visible inspection confirms this, and the differences are very clear, especially in the frame on the left, in which higher and lower thresholds return overlaid lanes with curvature offsets that are too high. This is likely because of the shadows present on crowded roads. A threshold that is too low doesn’t filter out noise.
Conversely, a threshold that is too high doesn’t allow us to capture the higher intensity pixels in the image associated with lane markings. So the threshold of 200 performs the best.

<h5>Dark Roads</h5>
<div style="text-align: center;">
<img style="height: 500px;" alt="" src="dark_classical.png">
</div>
<br>
Performance overall, with regards to F-score, for darks roads is the worst of all the conditions, as expected. Differences between F-scores with different thresholds are also rather small, as seen. Visible differences between outputs with different thresholds are also minimal. One reason for is that thresholding matters far less when lighting conditions are very low. More importantly, when lane detection approaches try to fit a polynomial to the transformed and filtered image, the presence of bright lens flares (caused by internal reflections when taking pictures with bright sources) in the image can throw off the CV thresholding approach. Clearer input images or further post-processing with flare-candidate filtering and flare mask computation[4] can significantly improve the lane fitting.

<h5>Optimal Conditions</h5>
<div style="text-align: center;">
<img style="height: 500px;" alt="" src="optimal_classical.png">
</div>
<br>
High thresholds performed the best for optimal conditions. Low thresholding once again offers less information for the fit to happen. Once a perspective transform is done and features of the image such as the sky are removed, long clusters of brighter pixels (associated with lanes) are identified as lanes. This result is expected because there is good lighting and no shadows due to the car’s surroundings. Higher thresholds allow extraction of pertinent clusters.

<h5>Curved Roads</h5>
<div style="text-align: center;">
<img style="height: 500px;" alt="" src="curved_classical.png">
</div>
<br>
This category also performed well, along with Optimal Conditions. The visible lane outputs are almost identical for different thresholds, as the F-scores corroborate. As the analysis above indicates, lighting conditions are good in the test images. The reasons above for optimal conditions apply here as well.
<br><br>
<h4> Machine Learning Approach </h4>
<h5> Crowded Roads </h5>
<div style="text-align: center;">
<img style="height: 200px;" alt="" src="crowd_input.png">
<img style="height: 200px;" alt="" src="crowd_output.png">
</div>
<br>
<h5> Dark Roads </h5>
<div style="text-align: center;">
<img style="height: 200px;" alt="" src="dark_input.png">
<img style="height: 200px;" alt="" src="dark_output.png">
</div>
<br>
<h5> Optimal Conditions </h5>
<div style="text-align: center;">
<img style="height: 200px;" alt="" src="optimal_input.png">
<img style="height: 200px;" alt="" src="optimal_output.png">
</div>
<br>
<h5> Curved Roads </h5>
<div style="text-align: center;">
<img style="height: 200px;" alt="" src="curved_input.png">
<img style="height: 200px;" alt="" src="curved_output.png">
</div>
<br>

<!-- Main Results Figure -->
<!-- <div style="text-align: center;">
<img style="height: 300px;" alt="" src="qual_results.png">
</div>
<br><br> -->
<h3> Conclusion and Future Work </h3>
Our project’s primary goal was to detect lanes on the road the car is driving on and we achieved that by employing two different approaches: classical (using binarized grayscale perspective transform with polynomial regression) and machine learning (using deep convolutional branched neural network). Another goal of this study was to compare the two approaches; we evaluated the approaches in 4 different conditions - optimal lanes, curved lanes, crowded lanes, and lanes in the dark - on the basis of speed and f-scores. For the classical model, we found that for different cases, the same binarization threshold gave varying accuracy. Hence, we tuned the classical model for each case and compared its performance across 3 thresholds as well. We further calculated the radius of curvature and reported it in the output frames.
<br><br>
Our original plan was to have a video input however we used image frames from the video as the input instead. We only used CULane Dataset as it could be broken into our planned test cases easily, had sufficient number of instances for each, and was well annotated. Finally, we were planning to use pixel wise sum of squared errors but ended up using F-score instead since we felt it was a better evaluation metric as it takes both precision and recall into account.
<br><br>
<h4> Future Work </h4>
We have found that the classical approach is more accurate in certain cases while the machine learning approach is more accurate in other cases. We can use this fact to build a more robust road lane detection system.  As mentioned earlier, current lane and degree of turning can be the first steps in building an steering system for an autonomous car since the car will be able to steer such that it stays in its current lane, while also adjusting to the curvature of the road.
<br><br>
Having said that, our models themselves can be made more robust.  The classical approach currently assumes the lanes to be white and hence is only able to detect lanes of a particular type. Removing color constraints and using hough transform will make the system more robust. As we show in our results, different cases require different thresholds and hence dynamic thresholding can also be used to make the system more robust in the general scenario.
<br><br>
Our current models were simulated on a standard laptop compared to specialized processing systems that are usually employed in autonomous cars. Running the same evaluation on such systems may give a more accurate metric of performance. Moreover, the CULane Dataset consists of image frames from camera mounted on 6 different cars driving on roads in China and as such our evaluation omits cases where the results may be affected due to difference in the automobile (trucks seem to vibrate more than cars for example) and the camera position etc. By testing the model on more diverse data or through an in-the-wild study would give more perspective on the behavior of the two approaches.
<br><br>
<h3> References </h3>
  <ol>
    <li> Neven, Davy, Bert De Brabandere, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool. "Towards End-to-End Lane Detection: an Instance Segmentation Approach." arXiv preprint arXiv:1802.05591 (2018). </li>

    <li> De Brabandere, Bert, Davy Neven, and Luc Van Gool. "Semantic instance segmentation with a discriminative loss function." arXiv preprint arXiv:1708.02551 (2017). </li>

    <li> Pan, X. (2018, February 1). Spatial As Deep: Spatial CNN for Traffic Scene Understanding. Retrieved from <a href = https://xingangpan.github.io/projects/CULane.html> https://xingangpan.github.io/projects/CULane.html  </a> </li>

    <li> Chabert, Floris. "Automated Lens Flare Removal." </li>
  </ol>

  <hr>
  <footer>
  <p>© Soham Gadgil, Ashwin Natarajan, Shaurye Aggarwal, William Xia, Abhishek Tumuluru, Mohit Chauhan</p>
  </footer>
</div>
</div>

<br><br>

</body></html>
